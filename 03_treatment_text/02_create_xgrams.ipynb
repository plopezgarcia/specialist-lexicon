{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Longitud de título (en tokens) por documento por especialidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_of_title(df):\n",
    "    \n",
    "    # creamos un diccionario 73363625: 30 (id del fichero, longitud en tokens)\n",
    "    dic = {}\n",
    "    \n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        file_id = row['id']\n",
    "        #abstract = row['abstract']\n",
    "        title = row['title']\n",
    "        \n",
    "        tokens_title = obtain_list_tokens(title)\n",
    "        \n",
    "        # añadimos al diccionario\n",
    "        dic[file_id] = len(tokens_title)\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file):\n",
    "    \n",
    "    # fichero de salida\n",
    "    fout = open(path_output_file, \"w\")\n",
    "    list_ids = []\n",
    "    \n",
    "    # recorremos las especialidades\n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        \n",
    "        # leemos el dataframe (id y titulo)\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        dic = get_long_of_title(df)\n",
    "        \n",
    "        for idfile, long in dic.items():\n",
    "\n",
    "            # si el id no lo hemos incluido antes, entonces escribimos en el fichero de salida\n",
    "            if not idfile in list_ids:\n",
    "\n",
    "                fout.write(str(idfile) + '\\t' + str(long) + '\\n')\n",
    "                list_ids.append(idfile)\n",
    "                \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "path_output_file = 'len_titles.txt'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crear fichero de vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(df):\n",
    "    \n",
    "    tokens_title = []\n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #file_id = row['id']\n",
    "        #abstract = row['abstract']\n",
    "        title = row['title']\n",
    "        \n",
    "        tokens_title.extend(obtain_list_tokens(title))\n",
    "    \n",
    "    return tokens_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file):\n",
    "    \n",
    "    # fichero de salida\n",
    "    fout = open(path_output_file, \"w\")\n",
    "    list_terms = []\n",
    "    cont_vocabulary = 0\n",
    "    tam_total_vocabulary = 0\n",
    "    \n",
    "    # recorremos las especialidades\n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        \n",
    "        #print(specialty_name_csv)\n",
    "        \n",
    "        # leemos el dataframe (id y titulo)\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        tokens = get_tokens(df)\n",
    "        tam_total_vocabulary += len(tokens)\n",
    "        \n",
    "        for term in tokens:\n",
    "            # si el término no lo hemos incluido antes, entonces escribimos en el fichero de salida\n",
    "            if not term in list_terms:\n",
    "\n",
    "                fout.write(str(term) + '\\n')\n",
    "                list_terms.append(term)\n",
    "                cont_vocabulary += 1\n",
    "    fout.close()\n",
    "    print(\"Longitud del vocabulario (sin repetidos):\", cont_vocabulary)\n",
    "    print(\"Longitud del vocabulario (con repetidos):\", tam_total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario (sin repetidos): 115129\n",
      "Longitud del vocabulario (con repetidos): 6468959\n"
     ]
    }
   ],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "path_output_file = 'vocabulary.txt'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear estructura para contener xgrams y el id del fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_lowercase(term):\n",
    "    if not term is None:\n",
    "        return term.lower()\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_sentences(phrase):\n",
    "    return sent_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dic_specialty(df):\n",
    "    \n",
    "    # nuevo diccionario para cada especialidad\n",
    "    dic_specialty = {}\n",
    "    \n",
    "    # para tener el total de docs de cada especialidad\n",
    "    list_docs_specialty = []\n",
    "    \n",
    "    # diccionario de términos\n",
    "    dic_terms = {}\n",
    "    \n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        file_id = row['id']\n",
    "        list_docs_specialty.append(file_id)\n",
    "        \n",
    "        \n",
    "        title = row['title']\n",
    "        sentences = obtain_list_sentences(title)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            #print(file_id, sentence)\n",
    "            \n",
    "            tokens = obtain_list_tokens(sentence)\n",
    "            #print(tokens)\n",
    "            \n",
    "            # UNIGRAMAS\n",
    "            for token in tokens:\n",
    "                token = change_to_lowercase(token)\n",
    "                \n",
    "                if not token in set(string.punctuation) and not token.isdigit():\n",
    "                    #print(\"1GRAM\", str(token))\n",
    "                    \n",
    "                    # creamos la tupla y la añadimos\n",
    "                    tup = (token)\n",
    "                    dic_terms.setdefault(tup, []).append(file_id)\n",
    "            \n",
    "            #BIGRAMAS\n",
    "            for w1, w2 in bigrams(tokens, pad_right=True, pad_left=True):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                \n",
    "                #print(\"2GRAM\", str(w1) + ' ' + str(w2) )\n",
    "                \n",
    "                # creamos la tupla y la añadimos\n",
    "                tup = (w1,w2)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "                \n",
    "            \n",
    "            #TRIGRAMAS\n",
    "            for w1, w2, w3 in trigrams(tokens, pad_right=True, pad_left=True):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                w3 = change_to_lowercase(w3)\n",
    "                \n",
    "                #print(\"3GRAM\", str(w1) + ' ' + str(w2) + ' ' + str(w3))\n",
    "                \n",
    "                # creamos la tupla y la añadimos\n",
    "                tup = (w1,w2,w3)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "    \n",
    "    dic_specialty['terms'] = dic_terms\n",
    "    dic_specialty['docs'] = list(set(list_docs_specialty))\n",
    "    \n",
    "    return dic_specialty\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract):\n",
    "    \n",
    "    dic_final = {}\n",
    "    \n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    # recorremos el listado de especialidades\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        specialty_name = specialty_name_csv.split(\".csv\")[0]\n",
    "        \n",
    "        # leemos el dataframe\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        print(\"Specialty: \", specialty_name_csv , ' --- Longitud del DF:', len(df))\n",
    "        \n",
    "        dic_specialty = generate_dic_specialty(df)\n",
    "\n",
    "        dic_final[specialty_name] = dic_specialty\n",
    "        \n",
    "    with open('dic_specialties.pkl', 'wb') as f:\n",
    "        pickle.dump(dic_final, f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtenemos los ids de documentos de cada especialidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_object(path):\n",
    "    pickle_in = open(path,\"rb\")\n",
    "    obj = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    print(\"Cargado el objeto\", path.split(\"/\")[- 1])\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_diccionary_specialties(dic_spe, path_out_file):\n",
    "    fout = open(path_out_file, \"w\")\n",
    "    \n",
    "    for specialty, dic in dic_spe.items():\n",
    "        \n",
    "        #print(\"Especialidad:\", specialty)\n",
    "        \n",
    "        list_doc_specialty = dic['docs']\n",
    "        fout.write(specialty + '\\t' + str(list_doc_specialty) + '\\n')\n",
    "    fout.close()\n",
    "    print(\"Fichero escrito: \", path_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargado el objeto dic_specialties.pkl\n"
     ]
    }
   ],
   "source": [
    "path_dic = 'dic_specialties.pkl'\n",
    "path_out_file = 'idfiles_by_specialty.txt'\n",
    "dic_spe = deserialize_object(path_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichero escrito:  idfiles_by_specialty.txt\n"
     ]
    }
   ],
   "source": [
    "read_diccionary_specialties(dic_spe, path_out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
