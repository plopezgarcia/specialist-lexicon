{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Longitud de título (en tokens) por documento por especialidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_of_title(df):\n",
    "    \n",
    "    # creamos un diccionario 73363625: 30 (id del fichero, longitud en tokens)\n",
    "    dic = {}\n",
    "    \n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        file_id = row['id']\n",
    "        #abstract = row['abstract']\n",
    "        title = row['title']\n",
    "        \n",
    "        tokens_title = obtain_list_tokens(title)\n",
    "        \n",
    "        # añadimos al diccionario\n",
    "        dic[file_id] = len(tokens_title)\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file):\n",
    "    \n",
    "    # fichero de salida\n",
    "    fout = open(path_output_file, \"w\")\n",
    "    list_ids = []\n",
    "    \n",
    "    # recorremos las especialidades\n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        \n",
    "        # leemos el dataframe (id y titulo)\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        dic = get_long_of_title(df)\n",
    "        \n",
    "        for idfile, long in dic.items():\n",
    "\n",
    "            # si el id no lo hemos incluido antes, entonces escribimos en el fichero de salida\n",
    "            if not idfile in list_ids:\n",
    "\n",
    "                fout.write(str(idfile) + '\\t' + str(long) + '\\n')\n",
    "                list_ids.append(idfile)\n",
    "                \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "path_output_file = 'len_titles.txt'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear estructura para contener xgrams y el id del fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk import word_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "spa_stopwords = stopwords.words('spanish')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenize_intratokens = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_lowercase(term):\n",
    "    if not term is None:\n",
    "        return term.lower()\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    \n",
    "    # juntar caracter como 75% o 65€\n",
    "    '''\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    '''\n",
    "    # separar caracteres españoles como: \"¿Cuándo\" -> \"¿\", \"Cuándo\"\n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "    \n",
    "    tokens = news_tokens   \n",
    "    \n",
    "    # separar caracteres entre dígitos: \"1837-1907\" -> \"1837\", \"-\", \"1907\"\n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "\n",
    "        # coprobamos que tiene el formato 1837-1907, no: ed-d69, 31/02/2018, ...\n",
    "        if re.match(\"\\d{4}-\\d{4}\", t):\n",
    "\n",
    "            tokens_intratoken = tokenize_intratokens.tokenize(t)\n",
    "            news_tokens.extend(tokens_intratoken)\n",
    "\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "        \n",
    "    tokens = news_tokens   \n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_sentence_with_lemma(phrase):\n",
    "    doc = nlp(phrase)\n",
    "    new_phrase_lemma = phrase\n",
    "    \n",
    "    for token in doc:\n",
    "        new_phrase_lemma = new_phrase_lemma.replace(token.text,  token.lemma_)\n",
    "    \n",
    "    return new_phrase_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\" or token.text == '!' or token.text == '?':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
    "\n",
    "def obtain_list_sentences(phrase):\n",
    "    doc = nlp(phrase)\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_stopword(tupla):\n",
    "    stopword_frecuency = 0\n",
    "    for token in list(tupla):\n",
    "        if token in spa_stopwords:\n",
    "            stopword_frecuency += 1\n",
    "    \n",
    "    return stopword_frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~«»¿¡``·‒–—―‘’‚“”„®©\n"
     ]
    }
   ],
   "source": [
    "punctuations = string.punctuation + '«»¿¡``·‒–—―‘’‚“”„®©'\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_punctuation(tupla):\n",
    "    punct_frecuency = 0\n",
    "    \n",
    "    for token in list(tupla):\n",
    "        if token in punctuations or token == '...' or token == \"''\":\n",
    "            punct_frecuency += 1\n",
    "    \n",
    "    return punct_frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dic_specialty(df, list_terms_total):\n",
    "    \n",
    "    # nuevo diccionario para cada especialidad\n",
    "    dic_specialty = {}\n",
    "    \n",
    "    # para tener el total de docs de cada especialidad\n",
    "    list_docs_specialty = []\n",
    "    \n",
    "    # diccionario de términos\n",
    "    dic_terms = {}\n",
    "    \n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        file_id = row['id']\n",
    "        list_docs_specialty.append(file_id)\n",
    "        \n",
    "        \n",
    "        title = row['title']\n",
    "        sentences = obtain_list_sentences(title)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            #print(file_id, sentence)\n",
    "            \n",
    "            #sentence = obtain_sentence_with_lemma(sentence)\n",
    "            tokens = obtain_list_tokens(sentence)\n",
    "            tokens = [token for token in tokens if len(token) > 0]\n",
    "            \n",
    "            #print(tokens)\n",
    "\n",
    "            # UNIGRAMAS\n",
    "            for token in tokens:\n",
    "                token = change_to_lowercase(token)\n",
    "                \n",
    "                tup = (token)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "                \n",
    "                tup = (token)\n",
    "                \n",
    "                if not token in punctuations:\n",
    "                    \n",
    "                    list_terms_total.append(token)\n",
    "                    \n",
    "                    #print(\"1GRAM\", str(tup))\n",
    "                    dic_terms.setdefault(tup, []).append(file_id)\n",
    "                \n",
    "            #BIGRAMAS\n",
    "            for w1, w2 in bigrams(tokens) : #, pad_right=False, pad_left=True):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                \n",
    "                tup = (w1,w2)\n",
    "                \n",
    "                # si hay algun stopword\n",
    "                # si hay más de un signo de puntuación\n",
    "                if not how_many_stopword(tup) >= 1 and not how_many_punctuation(tup) >= 1: \n",
    "                    \n",
    "                    #print(\"2GRAM\", tup)\n",
    "                    dic_terms.setdefault(tup, []).append(file_id)\n",
    "                #else:\n",
    "                #    print(\"2GRAM NOT\", tup)\n",
    "            \n",
    "            #TRIGRAMAS\n",
    "            for w1, w2, w3 in trigrams(tokens): #, pad_right=True, pad_left=True):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                w3 = change_to_lowercase(w3)\n",
    "                \n",
    "                tup = (w1,w2,w3)\n",
    "                \n",
    "                # si hay dos stopword\n",
    "                # si la 3º palabra es stopword\n",
    "                # si la 1º palabra es puntuacion\n",
    "                # si la 3º palabra es puntuacion\n",
    "                if not how_many_stopword(tup) >=2 and \\\n",
    "                    not w3 in spa_stopwords and \\\n",
    "                    not w1 in punctuations and not w1 == \"...\" and not w1 == \"''\" and \\\n",
    "                    not w3 in punctuations and not w3 == \"...\" and not w3 == \"''\":\n",
    "                    #print(\"3GRAM\", tup)\n",
    "                    dic_terms.setdefault(tup, []).append(file_id)\n",
    "                \n",
    "                #else:\n",
    "                #    print(\"3GRAM NOT\", tup)\n",
    "            '''\n",
    "            #4GRAMS     \n",
    "            for w1, w2, w3, w4 in ngrams(tokens,4):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                w3 = change_to_lowercase(w3)\n",
    "                w4 = change_to_lowercase(w4)\n",
    "                \n",
    "                #print(\"4GRAM\", str(w1) + ' ' + str(w2) + ' ' + str(w3) +  ' ' + str(w4))\n",
    "                \n",
    "                # creamos la tupla y la añadimos\n",
    "                tup = (w1,w2,w3,w4)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "            \n",
    "            #5GRAMS     \n",
    "            for w1, w2, w3, w4, w5 in ngrams(tokens,5):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                w3 = change_to_lowercase(w3)\n",
    "                w4 = change_to_lowercase(w4)\n",
    "                w5 = change_to_lowercase(w5)\n",
    "                \n",
    "                # creamos la tupla y la añadimos\n",
    "                tup = (w1,w2,w3,w4,w5)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "            '''\n",
    "            \n",
    "    dic_specialty['terms'] = dic_terms\n",
    "    dic_specialty['docs'] = list(set(list_docs_specialty))\n",
    "    \n",
    "    return dic_specialty, list_terms_total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract):\n",
    "    \n",
    "    dic_final = {}\n",
    "    list_terms_total = [] # Palabras en el corpus\n",
    "    \n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    # recorremos el listado de especialidades\n",
    "    for index, specialty_df in enumerate(list_df):\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        specialty_name = specialty_name_csv.split(\".csv\")[0]\n",
    "        \n",
    "        # leemos el dataframe\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        print(index, \"- specialty: \", specialty_name_csv , ' --- Longitud del DF:', len(df))\n",
    "        \n",
    "        dic_specialty, list_terms_total = generate_dic_specialty(df, list_terms_total)\n",
    "        \n",
    "        dic_final[specialty_name] = dic_specialty\n",
    "        \n",
    "    with open('dic_specialties_1_3_gramas.pkl', 'wb') as f:\n",
    "        pickle.dump(dic_final, f)\n",
    "    \n",
    "    with open(\"vocabulario.txt\", \"w\") as f:\n",
    "        for term in list_terms_total:\n",
    "            f.write(str(term) + '\\n')\n",
    "    \n",
    "    print(\"Nº de palabras en el corpus:\", len(list_terms_total))\n",
    "    print(\"Nº de palabras diferentes en el corpus:\", len(set(list_terms_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - specialty:  H02.403.340_general_practice.csv  --- Longitud del DF: 219\n",
      "1 - specialty:  H02.403.429.515_medical_oncology.csv  --- Longitud del DF: 24946\n",
      "2 - specialty:  H02.403.330_forensic_medicine.csv  --- Longitud del DF: 371\n",
      "3 - specialty:  H02.403.810.468_ophthalmology.csv  --- Longitud del DF: 4697\n",
      "4 - specialty:  H02.403.810.788_surgery_plastic.csv  --- Longitud del DF: 1174\n",
      "5 - specialty:  H02.403.429.480_infectious_disease_medicine.csv  --- Longitud del DF: 5283\n",
      "6 - specialty:  H02.403.600_neurology.csv  --- Longitud del DF: 23912\n",
      "7 - specialty:  H02.403.763_reproductive_medicine.csv  --- Longitud del DF: 1627\n",
      "8 - specialty:  H02.403.690_psychiatry.csv  --- Longitud del DF: 3071\n",
      "9 - specialty:  H02.403.429.730_rheumatology.csv  --- Longitud del DF: 3428\n",
      "10 - specialty:  H02.403.740_radiology.csv  --- Longitud del DF: 2084\n",
      "11 - specialty:  H02.403.044.500_immunochemistry.csv  --- Longitud del DF: 7490\n",
      "12 - specialty:  H02.403.429.580_nephrology.csv  --- Longitud del DF: 7601\n",
      "13 - specialty:  H02.403.670.500_perinatology.csv  --- Longitud del DF: 8584\n",
      "14 - specialty:  H02.403.830_sports_medicine.csv  --- Longitud del DF: 332\n",
      "15 - specialty:  H02.403.355_geriatrics.csv  --- Longitud del DF: 35350\n",
      "16 - specialty:  H02.403.810.494_orthopedics.csv  --- Longitud del DF: 1772\n",
      "17 - specialty:  H02.403.810.850_traumatology.csv  --- Longitud del DF: 6020\n",
      "18 - specialty:  H02.403.670.400_neonatology.csv  --- Longitud del DF: 10970\n",
      "19 - specialty:  H02.403.350_genetics_medical.csv  --- Longitud del DF: 6359\n",
      "20 - specialty:  H02.403.810.425_neurosurgery.csv  --- Longitud del DF: 2862\n",
      "21 - specialty:  H02.403.810.526_otolaryngology.csv  --- Longitud del DF: 4689\n",
      "22 - specialty:  H02.403.720.500_epidemiology.csv  --- Longitud del DF: 32571\n",
      "23 - specialty:  H02.403.650_pathology.csv  --- Longitud del DF: 20501\n",
      "24 - specialty:  H02.403.066_anesthesiology.csv  --- Longitud del DF: 2136\n",
      "25 - specialty:  H02.403.429.405_gastroenterology.csv  --- Longitud del DF: 14895\n",
      "26 - specialty:  H02.403.763.750_gynecology.csv  --- Longitud del DF: 7732\n",
      "27 - specialty:  H02.403.225_dermatology.csv  --- Longitud del DF: 9411\n",
      "28 - specialty:  H02.403.720.750_preventive_medicine.csv  --- Longitud del DF: 90929\n",
      "29 - specialty:  H02.403.429.675_pulmonary_medicine.csv  --- Longitud del DF: 2086\n",
      "30 - specialty:  H02.403.810.450_obstetrics.csv  --- Longitud del DF: 8591\n",
      "31 - specialty:  H02.403.429.445_hematology.csv  --- Longitud del DF: 7096\n",
      "32 - specialty:  H02.403.645_palliative_medicine.csv  --- Longitud del DF: 516\n",
      "33 - specialty:  H02.403.340.500_family_practice.csv  --- Longitud del DF: 6593\n",
      "34 - specialty:  H02.403.429_internal_medicine.csv  --- Longitud del DF: 5453\n",
      "35 - specialty:  H02.403.810_specialties_surgical.csv  --- Longitud del DF: 293\n",
      "36 - specialty:  H02.403.670_pediatrics.csv  --- Longitud del DF: 6508\n",
      "37 - specialty:  H02.403.810.300_general_surgery.csv  --- Longitud del DF: 31487\n",
      "38 - specialty:  H02.403.909_venereology.csv  --- Longitud del DF: 4944\n",
      "39 - specialty:  H02.403.044_allergy_and_immunology.csv  --- Longitud del DF: 452\n",
      "40 - specialty:  H02.403.500_military_medicine.csv  --- Longitud del DF: 214\n",
      "41 - specialty:  H02.403.680.600_rehabilitation.csv  --- Longitud del DF: 3194\n",
      "42 - specialty:  H02.403.740.500_nuclear_medicine.csv  --- Longitud del DF: 2091\n",
      "43 - specialty:  H02.403.810.803_thoracic_surgery.csv  --- Longitud del DF: 4863\n",
      "44 - specialty:  H02.403.810.860_urology.csv  --- Longitud del DF: 13844\n",
      "45 - specialty:  H02.403.429.163_cardiology.csv  --- Longitud del DF: 27356\n",
      "46 - specialty:  H02.403.879_tropical_medicine.csv  --- Longitud del DF: 973\n",
      "47 - specialty:  H02.403.680_physical_and_rehabilitation_medicine.csv  --- Longitud del DF: 282\n",
      "48 - specialty:  H02.403.429.323_endocrinology.csv  --- Longitud del DF: 11555\n",
      "49 - specialty:  H02.403.074_bariatric_medicine.csv  --- Longitud del DF: 234\n",
      "50 - specialty:  H02.403.690.150_community_psychiatry.csv  --- Longitud del DF: 6565\n",
      "Nº de palabras en el corpus: 5697544\n",
      "Nº de palabras diferentes en el corpus: 90184\n"
     ]
    }
   ],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtenemos los ids de documentos de cada especialidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_object(path):\n",
    "    pickle_in = open(path,\"rb\")\n",
    "    obj = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    print(\"Cargado el objeto\", path.split(\"/\")[- 1])\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_diccionary_specialties(dic_spe, path_out_file):\n",
    "    fout = open(path_out_file, \"w\")\n",
    "    \n",
    "    for specialty, dic in dic_spe.items():\n",
    "        \n",
    "        #print(\"Especialidad:\", specialty)\n",
    "        \n",
    "        list_doc_specialty = dic['docs']\n",
    "        fout.write(specialty + '\\t' + str(list_doc_specialty) + '\\n')\n",
    "    fout.close()\n",
    "    print(\"Fichero escrito: \", path_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargado el objeto dic_specialties.pkl\n"
     ]
    }
   ],
   "source": [
    "path_dic = 'dic_specialties.pkl'\n",
    "path_out_file = 'idfiles_by_specialty.txt'\n",
    "dic_spe = deserialize_object(path_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichero escrito:  idfiles_by_specialty.txt\n"
     ]
    }
   ],
   "source": [
    "read_diccionary_specialties(dic_spe, path_out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
