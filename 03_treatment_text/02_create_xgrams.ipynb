{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Longitud de título (en tokens) por documento por especialidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_of_title(df):\n",
    "    \n",
    "    # creamos un diccionario 73363625: 30 (id del fichero, longitud en tokens)\n",
    "    dic = {}\n",
    "    \n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        file_id = row['id']\n",
    "        #abstract = row['abstract']\n",
    "        title = row['title']\n",
    "        \n",
    "        tokens_title = obtain_list_tokens(title)\n",
    "        \n",
    "        # añadimos al diccionario\n",
    "        dic[file_id] = len(tokens_title)\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file):\n",
    "    \n",
    "    # fichero de salida\n",
    "    fout = open(path_output_file, \"w\")\n",
    "    list_ids = []\n",
    "    \n",
    "    # recorremos las especialidades\n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        \n",
    "        # leemos el dataframe (id y titulo)\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        dic = get_long_of_title(df)\n",
    "        \n",
    "        for idfile, long in dic.items():\n",
    "\n",
    "            # si el id no lo hemos incluido antes, entonces escribimos en el fichero de salida\n",
    "            if not idfile in list_ids:\n",
    "\n",
    "                fout.write(str(idfile) + '\\t' + str(long) + '\\n')\n",
    "                list_ids.append(idfile)\n",
    "                \n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "path_output_file = 'len_titles.txt'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crear fichero de vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(df):\n",
    "    \n",
    "    tokens_title = []\n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #file_id = row['id']\n",
    "        #abstract = row['abstract']\n",
    "        title = row['title']\n",
    "        \n",
    "        tokens_title.extend(obtain_list_tokens(title))\n",
    "    \n",
    "    return tokens_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file):\n",
    "    \n",
    "    # fichero de salida\n",
    "    fout = open(path_output_file, \"w\")\n",
    "    list_terms = []\n",
    "    cont_vocabulary = 0\n",
    "    tam_total_vocabulary = 0\n",
    "    \n",
    "    # recorremos las especialidades\n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        \n",
    "        #print(specialty_name_csv)\n",
    "        \n",
    "        # leemos el dataframe (id y titulo)\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        tokens = get_tokens(df)\n",
    "        tam_total_vocabulary += len(tokens)\n",
    "        \n",
    "        for term in tokens:\n",
    "            # si el término no lo hemos incluido antes, entonces escribimos en el fichero de salida\n",
    "            if not term in list_terms:\n",
    "\n",
    "                fout.write(str(term) + '\\n')\n",
    "                list_terms.append(term)\n",
    "                cont_vocabulary += 1\n",
    "    fout.close()\n",
    "    print(\"Longitud del vocabulario (sin repetidos):\", cont_vocabulary)\n",
    "    print(\"Longitud del vocabulario (con repetidos):\", tam_total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario (sin repetidos): 115129\n",
      "Longitud del vocabulario (con repetidos): 6468959\n"
     ]
    }
   ],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "path_output_file = 'vocabulary.txt'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract, path_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear estructura para contener xgrams y el id del fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_lowercase(term):\n",
    "    if not term is None:\n",
    "        return term.lower()\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_tokens(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "    \n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "            \n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_list_sentences(phrase):\n",
    "    return sent_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dic_specialty(df):\n",
    "    \n",
    "    # nuevo diccionario para cada especialidad\n",
    "    dic_specialty = {}\n",
    "    \n",
    "    # para tener el total de docs de cada especialidad\n",
    "    list_docs_specialty = []\n",
    "    \n",
    "    # diccionario de términos\n",
    "    dic_terms = {}\n",
    "    \n",
    "    # Iteración por filas del DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        file_id = row['id']\n",
    "        list_docs_specialty.append(file_id)\n",
    "        \n",
    "        \n",
    "        title = row['title']\n",
    "        sentences = obtain_list_sentences(title)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            #print(file_id, sentence)\n",
    "            \n",
    "            tokens = obtain_list_tokens(sentence)\n",
    "            #print(tokens)\n",
    "            \n",
    "            # UNIGRAMAS\n",
    "            for token in tokens:\n",
    "                token = change_to_lowercase(token)\n",
    "                \n",
    "                if not token in set(string.punctuation) and not token.isdigit():\n",
    "                    #print(\"1GRAM\", str(token))\n",
    "                    \n",
    "                    # creamos la tupla y la añadimos\n",
    "                    tup = (token)\n",
    "                    dic_terms.setdefault(tup, []).append(file_id)\n",
    "            \n",
    "            #BIGRAMAS\n",
    "            for w1, w2 in bigrams(tokens, pad_right=True, pad_left=True):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                \n",
    "                #print(\"2GRAM\", str(w1) + ' ' + str(w2) )\n",
    "                \n",
    "                # creamos la tupla y la añadimos\n",
    "                tup = (w1,w2)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "                \n",
    "            \n",
    "            #TRIGRAMAS\n",
    "            for w1, w2, w3 in trigrams(tokens, pad_right=True, pad_left=True):\n",
    "                w1 = change_to_lowercase(w1)\n",
    "                w2 = change_to_lowercase(w2)\n",
    "                w3 = change_to_lowercase(w3)\n",
    "                \n",
    "                #print(\"3GRAM\", str(w1) + ' ' + str(w2) + ' ' + str(w3))\n",
    "                \n",
    "                # creamos la tupla y la añadimos\n",
    "                tup = (w1,w2,w3)\n",
    "                dic_terms.setdefault(tup, []).append(file_id)\n",
    "    \n",
    "    dic_specialty['terms'] = dic_terms\n",
    "    dic_specialty['docs'] = list(set(list_docs_specialty))\n",
    "    \n",
    "    return dic_specialty\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_specialties_title_abstract(path_df_specialty_title_abstract):\n",
    "    \n",
    "    dic_final = {}\n",
    "    \n",
    "    list_df = os.listdir(path_df_specialty_title_abstract)\n",
    "    # recorremos el listado de especialidades\n",
    "    for specialty_df in list_df:\n",
    "        \n",
    "        specialty_df = os.path.join(path_df_specialty_title_abstract, specialty_df)\n",
    "        specialty_name_csv = specialty_df.split(\"/\")[-1]\n",
    "        specialty_name = specialty_name_csv.split(\".csv\")[0]\n",
    "        \n",
    "        # leemos el dataframe\n",
    "        df = pd.read_csv(specialty_df)\n",
    "        print(\"Specialty: \", specialty_name_csv , ' --- Longitud del DF:', len(df))\n",
    "        \n",
    "        dic_specialty = generate_dic_specialty(df)\n",
    "\n",
    "        dic_final[specialty_name] = dic_specialty\n",
    "        \n",
    "    with open('dic_specialties.pkl', 'wb') as f:\n",
    "        pickle.dump(dic_final, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specialty:  H02.403.340_general_practice.csv  --- Longitud del DF: 219\n",
      "Specialty:  H02.403.429.515_medical_oncology.csv  --- Longitud del DF: 24946\n",
      "Specialty:  H02.403.330_forensic_medicine.csv  --- Longitud del DF: 371\n",
      "Specialty:  H02.403.810.468_ophthalmology.csv  --- Longitud del DF: 4697\n",
      "Specialty:  H02.403.810.788_surgery_plastic.csv  --- Longitud del DF: 1174\n",
      "Specialty:  H02.403.429.480_infectious_disease_medicine.csv  --- Longitud del DF: 5283\n",
      "Specialty:  H02.403.600_neurology.csv  --- Longitud del DF: 23912\n",
      "Specialty:  H02.403.763_reproductive_medicine.csv  --- Longitud del DF: 1627\n",
      "Specialty:  H02.403.690_psychiatry.csv  --- Longitud del DF: 3071\n",
      "Specialty:  H02.403.429.730_rheumatology.csv  --- Longitud del DF: 3428\n",
      "Specialty:  H02.403.740_radiology.csv  --- Longitud del DF: 2084\n",
      "Specialty:  H02.403.044.500_immunochemistry.csv  --- Longitud del DF: 7490\n",
      "Specialty:  H02.403.429.580_nephrology.csv  --- Longitud del DF: 7601\n",
      "Specialty:  H02.403.670.500_perinatology.csv  --- Longitud del DF: 8584\n",
      "Specialty:  H02.403.830_sports_medicine.csv  --- Longitud del DF: 332\n",
      "Specialty:  H02.403.355_geriatrics.csv  --- Longitud del DF: 35350\n",
      "Specialty:  H02.403.810.494_orthopedics.csv  --- Longitud del DF: 1772\n",
      "Specialty:  H02.403.810.850_traumatology.csv  --- Longitud del DF: 6020\n",
      "Specialty:  H02.403.670.400_neonatology.csv  --- Longitud del DF: 10970\n",
      "Specialty:  H02.403.350_genetics_medical.csv  --- Longitud del DF: 6359\n",
      "Specialty:  H02.403.810.425_neurosurgery.csv  --- Longitud del DF: 2862\n",
      "Specialty:  H02.403.810.526_otolaryngology.csv  --- Longitud del DF: 4689\n",
      "Specialty:  H02.403.720.500_epidemiology.csv  --- Longitud del DF: 32571\n",
      "Specialty:  H02.403.650_pathology.csv  --- Longitud del DF: 20501\n",
      "Specialty:  H02.403.066_anesthesiology.csv  --- Longitud del DF: 2136\n",
      "Specialty:  H02.403.429.405_gastroenterology.csv  --- Longitud del DF: 14895\n",
      "Specialty:  H02.403.763.750_gynecology.csv  --- Longitud del DF: 7732\n",
      "Specialty:  H02.403.225_dermatology.csv  --- Longitud del DF: 9411\n",
      "Specialty:  H02.403.720.750_preventive_medicine.csv  --- Longitud del DF: 90929\n",
      "Specialty:  H02.403.429.675_pulmonary_medicine.csv  --- Longitud del DF: 2086\n",
      "Specialty:  H02.403.810.450_obstetrics.csv  --- Longitud del DF: 8591\n",
      "Specialty:  H02.403.429.445_hematology.csv  --- Longitud del DF: 7096\n",
      "Specialty:  H02.403.645_palliative_medicine.csv  --- Longitud del DF: 516\n",
      "Specialty:  H02.403.340.500_family_practice.csv  --- Longitud del DF: 6593\n",
      "Specialty:  H02.403.429_internal_medicine.csv  --- Longitud del DF: 5453\n",
      "Specialty:  H02.403.810_specialties_surgical.csv  --- Longitud del DF: 293\n",
      "Specialty:  H02.403.670_pediatrics.csv  --- Longitud del DF: 6508\n",
      "Specialty:  H02.403.810.300_general_surgery.csv  --- Longitud del DF: 31487\n",
      "Specialty:  H02.403.909_venereology.csv  --- Longitud del DF: 4944\n",
      "Specialty:  H02.403.044_allergy_and_immunology.csv  --- Longitud del DF: 452\n",
      "Specialty:  H02.403.500_military_medicine.csv  --- Longitud del DF: 214\n",
      "Specialty:  H02.403.680.600_rehabilitation.csv  --- Longitud del DF: 3194\n",
      "Specialty:  H02.403.740.500_nuclear_medicine.csv  --- Longitud del DF: 2091\n",
      "Specialty:  H02.403.810.803_thoracic_surgery.csv  --- Longitud del DF: 4863\n",
      "Specialty:  H02.403.810.860_urology.csv  --- Longitud del DF: 13844\n",
      "Specialty:  H02.403.429.163_cardiology.csv  --- Longitud del DF: 27356\n",
      "Specialty:  H02.403.879_tropical_medicine.csv  --- Longitud del DF: 973\n",
      "Specialty:  H02.403.680_physical_and_rehabilitation_medicine.csv  --- Longitud del DF: 282\n",
      "Specialty:  H02.403.429.323_endocrinology.csv  --- Longitud del DF: 11555\n",
      "Specialty:  H02.403.074_bariatric_medicine.csv  --- Longitud del DF: 234\n",
      "Specialty:  H02.403.690.150_community_psychiatry.csv  --- Longitud del DF: 6565\n"
     ]
    }
   ],
   "source": [
    "path_df_specialty_title_abstract = './dataframes/df_specialty_title_abstract'\n",
    "read_df_specialties_title_abstract(path_df_specialty_title_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtenemos los ids de documentos de cada especialidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_object(path):\n",
    "    pickle_in = open(path,\"rb\")\n",
    "    obj = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    print(\"Cargado el objeto\", path.split(\"/\")[- 1])\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_diccionary_specialties(dic_spe, path_out_file):\n",
    "    fout = open(path_out_file, \"w\")\n",
    "    \n",
    "    for specialty, dic in dic_spe.items():\n",
    "        \n",
    "        #print(\"Especialidad:\", specialty)\n",
    "        \n",
    "        list_doc_specialty = dic['docs']\n",
    "        fout.write(specialty + '\\t' + str(list_doc_specialty) + '\\n')\n",
    "    fout.close()\n",
    "    print(\"Fichero escrito: \", path_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargado el objeto dic_specialties.pkl\n"
     ]
    }
   ],
   "source": [
    "path_dic = 'dic_specialties.pkl'\n",
    "path_out_file = 'idfiles_by_specialty.txt'\n",
    "dic_spe = deserialize_object(path_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichero escrito:  idfiles_by_specialty.txt\n"
     ]
    }
   ],
   "source": [
    "read_diccionary_specialties(dic_spe, path_out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
