{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtenemos la siguientes medidas:\n",
    "\n",
    "### Obtenemos las siguientes métricas:\n",
    "- P(e|d)\n",
    "- P(d|e)\n",
    "- ICC\n",
    "- TScore\n",
    "- IDF\n",
    "- IDF normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_object(path):\n",
    "    pickle_in = open(path,\"rb\")\n",
    "    obj = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    print(\"Cargado el objeto\", path.split(\"/\")[- 1])\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Obtenemos el nº de documentos del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dic_spe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-456dc660f92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_docs_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mspecialty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic_spe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlist_doc_total_specialty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'docs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlist_docs_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_doc_total_specialty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dic_spe' is not defined"
     ]
    }
   ],
   "source": [
    "list_docs_total = []\n",
    "for specialty, dic in dic_spe.items():\n",
    "    list_doc_total_specialty = dic['docs']\n",
    "    \n",
    "    list_docs_total.extend(list_doc_total_specialty)\n",
    "\n",
    "print(\"Nº de documentos totales:\", len(set(list_docs_total)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Obtenemos las métricas de cada término"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nº de documentos del corpus\n",
    "num_total_docs_corpus = 195234\n",
    "\n",
    "#Nº de palabras en el corpus: 6469046\n",
    "#Nº de palabras diferentes en el corpus: 115121\n",
    "N = 6469046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nº de documentos en la especialidad que NO tienen el n-gram\n",
    "def obtain_ndpe(list_doc_term, list_doc_total_spe):\n",
    "    return len(set(set(list_doc_total_spe)-set(list_doc_term)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et: nº de especialidades que contienen el ngram\n",
    "def obtain_et(term, dic_spe):\n",
    "    et = 0\n",
    "\n",
    "    for specialty, dic in dic_spe.items():\n",
    "        \n",
    "        # obtenemos el diccionario de términos\n",
    "        dic_term = dic['terms']\n",
    "        \n",
    "         # si el término se encuentra en el diccionario sumamos 1\n",
    "        if term in dic_term:\n",
    "            et += 1\n",
    "            \n",
    "    return et      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de documentos en las OTRAS especialidades que tienen el n-gram\n",
    "def obtain_ndep(term, dic_spe, specialty_actual):\n",
    "    \n",
    "    ndpe = 0\n",
    "\n",
    "    for specialty, dic in dic_spe.items():\n",
    "        \n",
    "        # si la especialidad no es la misma\n",
    "        if not specialty == specialty_actual:\n",
    "            \n",
    "            # obtenemos el diccionario de términos\n",
    "            dic_term = dic['terms']\n",
    "            \n",
    "            # si el término se encuentra en el diccionario\n",
    "            if term in dic_term:\n",
    "                \n",
    "                # obtenemos los documentos en los que aparece\n",
    "                doc_term = list(set(dic_term[term]))\n",
    "                \n",
    "                # contamos el nº de documentos y sumamos\n",
    "                ndpe += len(doc_term)\n",
    "                \n",
    "    return ndpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos las veces que se repite el termino en el corpus\n",
    "def obtain_nocc_term(term, dic_spe):\n",
    "    \n",
    "    nocc_term = 0\n",
    "\n",
    "    for specialty, dic in dic_spe.items():\n",
    "        # obtenemos el diccionario de términos\n",
    "        dic_term = dic['terms']\n",
    "\n",
    "        # si el término se encuentra en el diccionario\n",
    "        if term in dic_term:\n",
    "\n",
    "            # obtenemos el nº los documentos en los que aparece\n",
    "            nocc_term += len(dic_term[term])\n",
    "                \n",
    "    return nocc_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-score\n",
    "def obtain_tscore(term, dic_spe):\n",
    "    \n",
    "    # unigramas\n",
    "    if not type(term) is tuple:\n",
    "        nocc_term = obtain_nocc_term(term, dic_spe)\n",
    "        \n",
    "        ts = nocc_term - nocc_term / math.sqrt(nocc_term / N)\n",
    "        \n",
    "        return ts, nocc_term, 0, 0, 0\n",
    "       \n",
    "    else:\n",
    "        # brigramas\n",
    "        if len(term) == 2:\n",
    "            nocc_bigram = obtain_nocc_term(term, dic_spe)\n",
    "            nocc_w1 = obtain_nocc_term(term[0], dic_spe)\n",
    "            nocc_w2 = obtain_nocc_term(term[1], dic_spe)\n",
    "        \n",
    "            ts = nocc_bigram - (nocc_w1 * nocc_w2) / math.sqrt(nocc_bigram / N)\n",
    "            \n",
    "            return ts, nocc_bigram, nocc_w1, nocc_w2, 0\n",
    "            \n",
    "        # trigramas\n",
    "        elif len(term) == 3:\n",
    "            \n",
    "            nocc_trigram = obtain_nocc_term(term, dic_spe)\n",
    "            nocc_w1 = obtain_nocc_term(term[0], dic_spe)\n",
    "            nocc_w2 = obtain_nocc_term(term[1], dic_spe)\n",
    "            nocc_w3 = obtain_nocc_term(term[2], dic_spe)\n",
    "            \n",
    "            ts =  ( nocc_trigram - (nocc_w1 * nocc_w2 * nocc_w3) ) / math.sqrt(nocc_trigram / N)\n",
    "        \n",
    "            return ts, nocc_trigram, nocc_w1, nocc_w2, nocc_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_n_of_ngram(term):\n",
    "    if not type(term) is tuple:\n",
    "        return 1\n",
    "    else:\n",
    "        if len(term) == 2:\n",
    "            return 2\n",
    "        elif len(term) == 3:\n",
    "            return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_terms(path_out, dic_spe):\n",
    "    \n",
    "    #list_spe_import = ['H02.403.680.600_rehabilitation', 'H02.403.225_dermatology', 'H02.403.763.750_gynecology', 'H02.403.740_radiology']\n",
    "    #list_spe_import = ['H02.403.340.500_family_practice']\n",
    "    for specialty, dic in dic_spe.items():\n",
    "        \n",
    "        #if specialty in list_spe_import:\n",
    "        if not os.path.exists(path_out + specialty + \"_terms_weight.csv\"):  \n",
    "            with open(path_out + specialty + \"_terms_weight.csv\", \"w\") as fout:\n",
    "\n",
    "                list_doc_total_specialty = dic['docs']\n",
    "                dic_terms = dic['terms']\n",
    "\n",
    "                print(\"Especialidad:\", specialty, 'nº docs total:', len(list_doc_total_specialty))\n",
    "\n",
    "                # primero obtenemos el idf de todos los términos para calcular la media y desviación\n",
    "                # para posteriormente calcular el idf-normalizado\n",
    "                list_idfs = []\n",
    "\n",
    "                for term, list_doc_term in dic_terms.items():\n",
    "\n",
    "                    nde = len(set(list_doc_term))\n",
    "                    ndep = obtain_ndep(term, dic_spe, specialty)\n",
    "\n",
    "                    num_doc_term = nde + ndep\n",
    "\n",
    "                    idf = math.log2(num_total_docs_corpus / num_doc_term)\n",
    "\n",
    "                    list_idfs.append(idf)\n",
    "\n",
    "                mean_idfs = statistics.mean(list_idfs)\n",
    "                desv_idfs = statistics.stdev(list_idfs)\n",
    "\n",
    "                '''\n",
    "                    Creamos las nuevas métricas\n",
    "                '''\n",
    "\n",
    "                fout.write(\"Término|\")\n",
    "\n",
    "                fout.write(\"Ngram|\")\n",
    "\n",
    "                fout.write(\"N(d,e)|\")\n",
    "                fout.write(\"N(d,e')|\")\n",
    "                fout.write(\"N(d',e)|\")\n",
    "\n",
    "                fout.write(\"P(e,d)|\")\n",
    "                fout.write(\"P(d,e)|\")\n",
    "\n",
    "                fout.write(\"SpecialtyWeight|\")\n",
    "\n",
    "                fout.write(\"Idf|\")\n",
    "                fout.write(\"Idf-norm|\")\n",
    "\n",
    "                fout.write(\"SpecialtyWeight*Idf-norm|\")\n",
    "\n",
    "                fout.write(\"\\n\")\n",
    "\n",
    "                for term, list_doc_term in dic_terms.items():\n",
    "\n",
    "                    num_ooc_spe = len(list_doc_term)\n",
    "\n",
    "                    nde = len(set(list_doc_term))\n",
    "\n",
    "                    ndep = obtain_ndep(term, dic_spe, specialty)\n",
    "                    ndpe = obtain_ndpe(list_doc_term, list_doc_total_specialty)\n",
    "                    #et = obtain_et(term, dic_spe)\n",
    "\n",
    "                    # p(e|d)\n",
    "                    ped = nde / (nde + ndep)\n",
    "\n",
    "                    # p(d|e)\n",
    "                    pde = nde / (nde + ndpe)\n",
    "\n",
    "                    # specialty level weight\n",
    "                    slw = ped + pde\n",
    "\n",
    "                    # idf\n",
    "                    num_doc_term = nde + ndep\n",
    "                    idf = math.log2(num_total_docs_corpus / num_doc_term)\n",
    "\n",
    "                    # idf-norml\n",
    "                    idf_norm = (idf - mean_idfs) / desv_idfs\n",
    "\n",
    "                    # specialty level weight * idf-norml\n",
    "                    tw = slw * idf_norm\n",
    "\n",
    "                    # icc\n",
    "                    #icc = 1 /  et\n",
    "\n",
    "                    # t-score\n",
    "                    #tscore, nocc_term, nocc_w1, nocc_w2, nocc_w3 = obtain_tscore(term, dic_spe)\n",
    "                    #print(nocc_term, nocc_w1, nocc_w2, nocc_w3)\n",
    "\n",
    "                    # corpus level weight\n",
    "                    #clw = icc * tscore\n",
    "\n",
    "                    # total weight\n",
    "                    #tw = clw * slw\n",
    "\n",
    "                    # otra prueba (icc x specialty level weight)\n",
    "                    #other_total = icc * slw\n",
    "\n",
    "                    # imprimir en fichero\n",
    "                    fout.write(str(term) + \"|\")\n",
    "\n",
    "                    # si es 1 gram 2 gram o 3 gram\n",
    "                    fout.write(str(obtain_n_of_ngram(term)) + '|')\n",
    "\n",
    "                    fout.write(str(nde) +'|')\n",
    "                    fout.write(str(ndep)+ '|')\n",
    "                    fout.write(str(ndpe) + '|')\n",
    "\n",
    "                    fout.write(str(ped)+ '|')\n",
    "                    fout.write(str(pde)+ '|')\n",
    "\n",
    "                    fout.write(str(slw) + \"|\")\n",
    "\n",
    "                    fout.write(str(idf) + \"|\")\n",
    "                    fout.write(str(idf_norm) + \"|\")\n",
    "\n",
    "                    fout.write(str(tw) + \"|\")\n",
    "\n",
    "                    fout.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargado el objeto dic_specialties_lemma.pkl\n"
     ]
    }
   ],
   "source": [
    "path_dic = 'dic_specialties.pkl'\n",
    "path_dic_lemma = 'dic_specialties_lemma.pkl'\n",
    "\n",
    "dic_spe = deserialize_object(path_dic_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Especialidad: H02.403.720.750_preventive_medicine nº docs total: 83263\n"
     ]
    }
   ],
   "source": [
    "#path_output = './term_weight/1_3grams/'\n",
    "#path_output = './term_weight/1_5grams/'\n",
    "path_output = './term_weight/1_3grams_lemma/'\n",
    "\n",
    "calculate_weight_terms(path_output, dic_spe)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
