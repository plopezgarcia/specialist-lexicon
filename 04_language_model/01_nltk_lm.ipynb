{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK with Language Model\n",
    "\n",
    "https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/test/unit/lm/test_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.models import Laplace, KneserNeyInterpolated, WittenBellInterpolated\n",
    "import os\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(phrase):\n",
    "    tokens = word_tokenize(phrase, language='spanish')\n",
    "    i_offset = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        i -= i_offset\n",
    "        if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "            left = tokens[:i-1]\n",
    "            joined = [tokens[i - 1] + t]\n",
    "            right = tokens[i + 1:]\n",
    "            tokens = left + joined + right\n",
    "            i_offset += 1\n",
    "\n",
    "    news_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.startswith('¿') or t.startswith('¡'):\n",
    "            news_tokens.append(t[0])\n",
    "            news_tokens.append(t[1:])\n",
    "        else:\n",
    "            news_tokens.append(t)\n",
    "\n",
    "    return news_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(csv):\n",
    "    data = pd.read_csv(csv)\n",
    "    list_titles = data.title.values\n",
    "    \n",
    "    list_titles_return = []\n",
    "    \n",
    "    for title in list_titles:\n",
    "       \n",
    "        tokens_title = tokenize(title)\n",
    "        tokens_title = [token for token in tokens_title if not token.isdigit()]\n",
    "        tokens_title = [token.lower() for token in tokens_title if not token in string.punctuation]\n",
    "\n",
    "        list_titles_return.append(tokens_title)\n",
    "    \n",
    "    return list_titles_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train                          Test                      Laplace1Gram   Laplace2Gram   Laplace3Gram\n",
      "\n",
      "medical_oncology.csv           general_practice.csv      1403.805670   1385.993641   1257.148111   \n",
      "medical_oncology.csv           medical_oncology.csv      4653.949746   "
     ]
    }
   ],
   "source": [
    "train_sentences = [['an', 'apple'], ['an', 'orange']]\n",
    "test_sentences = [['an', 'an'], [ \"orange\"]]\n",
    "\n",
    "\n",
    "files_train = '../03_treatment_text/dataframes/df_specialty_title_abstract/'\n",
    "files_test = '../03_treatment_text/dataframes/df_specialty_title_abstract_case_report/'\n",
    "\n",
    "for file_train in os.listdir(files_train):\n",
    "\n",
    "    if not 'general_practice' in file_train:\n",
    "    \n",
    "        train_sentences = prepare_data(files_train + file_train)\n",
    "\n",
    "        print('\\n{:30} {:25} {:6s}   {:6s}   {:6s}'.format(\"Train\", \"Test\", \"Laplace1Gram\", \"Laplace2Gram\", \"Laplace3Gram\"))\n",
    "        for file_test in os.listdir(files_test):\n",
    "\n",
    "            print('\\n{:30} {:25} '.format(file_train.split(\"_\", 1)[1], file_test.split(\"_\", 1)[1])  , end = '')\n",
    "\n",
    "            test_sentences = prepare_data(files_test + file_test)\n",
    "\n",
    "\n",
    "            for n in range(1, 4):\n",
    "\n",
    "                '''\n",
    "                    MLE\n",
    "                train_data, padded_vocab = padded_everygram_pipeline(n, train_sentences)\n",
    "                test_data, _ = padded_everygram_pipeline(n, test_sentences)\n",
    "\n",
    "                # Lets train a N-grams model, previously we set n=1 or 2 or 3\n",
    "                model = MLE(n) # MLE / Laplace add1 / KneserNeyInterpolated / WittenBellInterpolated / \n",
    "                # fit on padded vocab that the model know the new tokens added to vocab (<s>, </s>, UNK etc)\n",
    "                model.fit(train_data, padded_vocab)\n",
    "\n",
    "                sum = 0\n",
    "                cont = 0\n",
    "                for index, test in enumerate(test_data):\n",
    "                    pp = model.perplexity(test)\n",
    "\n",
    "                    #print(index, test_sentences[index])\n",
    "                    #print(\"PP (model MLE) {}-gram: {}\".format( n , pp))\n",
    "\n",
    "                    cont += 1\n",
    "                    sum += pp\n",
    "\n",
    "                print(\"PP (model MLE) {}-gram: {}\".format( n , sum/cont))\n",
    "                '''\n",
    "\n",
    "\n",
    "\n",
    "                '''\n",
    "                    Lappace\n",
    "                '''\n",
    "\n",
    "                train_data, padded_vocab = padded_everygram_pipeline(n, train_sentences)\n",
    "                test_data, _ = padded_everygram_pipeline(n, test_sentences)\n",
    "\n",
    "                model = Laplace(n) # MLE / Laplace add1 / KneserNeyInterpolated / WittenBellInterpolated / \n",
    "                model.fit(train_data, padded_vocab)\n",
    "\n",
    "                sum = 0\n",
    "                cont = 0\n",
    "                for index, test in enumerate(test_data):\n",
    "                    pp = model.perplexity(test)\n",
    "                    cont += 1\n",
    "                    sum += pp\n",
    "\n",
    "                print(\"{:6f}   \".format(sum/cont) , end = '')\n",
    "\n",
    "\n",
    "                '''\n",
    "                    KneserNeyInterpolated\n",
    "\n",
    "\n",
    "                train_data, padded_vocab = padded_everygram_pipeline(n, train_sentences)\n",
    "                test_data, _ = padded_everygram_pipeline(n, test_sentences)\n",
    "\n",
    "                model = KneserNeyInterpolated(n) # MLE / Laplace add1 / KneserNeyInterpolated / WittenBellInterpolated / \n",
    "                model.fit(train_data, padded_vocab)\n",
    "\n",
    "                sum = 0\n",
    "                cont = 0\n",
    "                for index, test in enumerate(test_data):\n",
    "                    try:\n",
    "                        pp = model.perplexity(test)\n",
    "                    except ZeroDivisionError:\n",
    "                        pp = float('inf')\n",
    "\n",
    "                    cont += 1\n",
    "                    sum += pp\n",
    "\n",
    "                print(\"PP (model KneserNeyInterpolated) {}-gram: {}\".format( n , sum/cont))\n",
    "\n",
    "                '''\n",
    "                    #WittenBellInterpolated\n",
    "                '''\n",
    "\n",
    "                train_data, padded_vocab = padded_everygram_pipeline(n, train_sentences)\n",
    "                test_data, _ = padded_everygram_pipeline(n, test_sentences)\n",
    "\n",
    "                model = WittenBellInterpolated(n) # MLE / Laplace add1 / KneserNeyInterpolated / WittenBellInterpolated / \n",
    "                model.fit(train_data, padded_vocab)\n",
    "\n",
    "                sum = 0\n",
    "                cont = 0\n",
    "                for index, test in enumerate(test_data):\n",
    "                    pp = model.perplexity(test)\n",
    "                    cont += 1\n",
    "                    sum += pp\n",
    "\n",
    "                print(\"PP (model WittenBellInterpolated) {}-gram: {}\".format( n , sum/cont))\n",
    "                '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
