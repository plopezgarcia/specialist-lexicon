{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math as calc\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nGramProbability():\n",
    "    \"\"\"A program which creates n-Gram (1-5) Maximum Likelihood Probabilistic Language Model with Laplace Add-1 smoothing\n",
    "    and stores it in hash-able dictionary form.\n",
    "    n: number of bigrams (supports up to 5)\n",
    "    corpus_file: relative path to the corpus file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=1, corpus_file=None):\n",
    "        \"\"\"Constructor method which loads the corpus from file and creates ngrams based on imput parameters.\"\"\"\n",
    "        self.words = []\n",
    "        self.load_corpus(corpus_file)\n",
    "        self.unigram = self.bigram = self.trigram = self.quadrigram = self.pentigram = None\n",
    "        self.create_unigram()\n",
    "        if n >= 2:\n",
    "            self.create_bigram()\n",
    "        if n >= 3:\n",
    "            self.create_trigram()\n",
    "        if n >= 4:\n",
    "            self.create_quadrigram()\n",
    "        if n >= 5:\n",
    "            self.create_pentigram()\n",
    "        return\n",
    "\n",
    "    def tokenize(self, phrase):\n",
    "        tokens = word_tokenize(phrase, language='spanish')\n",
    "        i_offset = 0\n",
    "        for i, t in enumerate(tokens):\n",
    "            i -= i_offset\n",
    "            if (t == '%' or t == '$' or t == '€')and i > 0:\n",
    "                left = tokens[:i-1]\n",
    "                joined = [tokens[i - 1] + t]\n",
    "                right = tokens[i + 1:]\n",
    "                tokens = left + joined + right\n",
    "                i_offset += 1\n",
    "        \n",
    "        news_tokens = []\n",
    "        for t in tokens:\n",
    "            if t.startswith('¿') or t.startswith('¡'):\n",
    "                news_tokens.append(t[0])\n",
    "                news_tokens.append(t[1:])\n",
    "            else:\n",
    "                news_tokens.append(t)\n",
    "                \n",
    "        return news_tokens\n",
    "\n",
    "    def read_sentences_from_file(self, file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        list_titles = data.title.values\n",
    "        list_tokens_return = []\n",
    "        \n",
    "        for title in list_titles:\n",
    "\n",
    "            # pre-processing text (remove digits, remove punctuations, lowercase)\n",
    "            list_tokens = self.tokenize(title)\n",
    "            list_tokens = [token for token in list_tokens if not token.isdigit()]\n",
    "            list_tokens = [token.lower() for token in list_tokens if not token in string.punctuation]\n",
    "            \n",
    "            list_tokens.insert(0, \"<s>\")\n",
    "            list_tokens.append(\"</s>\")\n",
    "\n",
    "            list_tokens_return.extend(list_tokens)\n",
    "            \n",
    "        return list_tokens_return\n",
    "\n",
    "    def load_corpus(self, file_name):\n",
    "        \"\"\"Method to load external file which contains raw corpus.\"\"\"\n",
    "        print(\"Loading Corpus from data file\")\n",
    "        self.words = self.read_sentences_from_file(file_name)\n",
    "        print(\"Processing Corpus\")\n",
    "    \n",
    "    def create_unigram(self):\n",
    "        \"\"\"Method to create Unigram Model for words loaded from corpus.\"\"\"\n",
    "        print(\"Creating Unigram Model\")\n",
    "        unigram_file = None\n",
    "        print(\"Calculating Count for Unigram Model\")\n",
    "        unigram = Counter(self.words)\n",
    "        self.unigram = unigram\n",
    "\n",
    "    def create_bigram(self):\n",
    "        \"\"\"Method to create Bigram Model for words loaded from corpus.\"\"\"\n",
    "        print(\"Creating Bigram Model\")\n",
    "        words = self.words\n",
    "        biwords = []\n",
    "        for index, item in enumerate(words):\n",
    "            if index == len(words)-1:\n",
    "                break\n",
    "            biwords.append(item+' '+words[index+1])\n",
    "        print(\"Calculating Count for Bigram Model\")\n",
    "        bigram_file = None\n",
    "        bigram = Counter(biwords)\n",
    "        self.bigram = bigram\n",
    "\n",
    "    def create_trigram(self):\n",
    "        \"\"\"Method to create Trigram Model for words loaded from corpus.\"\"\"\n",
    "        print(\"Creating Trigram Model\")\n",
    "        words = self.words\n",
    "        triwords = []\n",
    "        for index, item in enumerate(words):\n",
    "            if index == len(words)-2:\n",
    "                break\n",
    "            triwords.append(item+' '+words[index+1]+' '+words[index+2])\n",
    "        print(\"Calculating Count for Trigram Model\")\n",
    "        trigram = Counter(triwords)\n",
    "        self.trigram = trigram\n",
    "\n",
    "    def create_quadrigram(self):\n",
    "        \"\"\"Method to create Quadrigram Model for words loaded from corpus.\"\"\"\n",
    "        print(\"Creating Quadrigram Model\")\n",
    "        words = self.words\n",
    "        quadriwords = []\n",
    "        for index, item in enumerate(words):\n",
    "            if index == len(words)-3:\n",
    "                break\n",
    "            quadriwords.append(item+' '+words[index+1]+' '+words[index+2]+' '+words[index+3])\n",
    "        print(\"Calculating Count for Quadrigram Model\")\n",
    "        quadrigram = Counter(quadriwords)\n",
    "        self.quadrigram = quadrigram\n",
    "\n",
    "    def create_pentigram(self):\n",
    "        \"\"\"Method to create Pentigram Model for words loaded from corpus.\"\"\"\n",
    "        print(\"Creating pentigram Model\")\n",
    "        words = self.words\n",
    "        pentiwords = []\n",
    "        for index, item in enumerate(words):\n",
    "            if index == len(words)-4:\n",
    "                break\n",
    "            pentiwords.append(item+' '+words[index+1]+' '+words[index+2]+' '+words[index+3]+' '+words[index+4])\n",
    "        print(\"Calculating Count for pentigram Model\")\n",
    "        pentigram = Counter(pentiwords)\n",
    "        self.pentigram = pentigram\n",
    "\n",
    "    def sentence_probability_mle(self, sentence, n=1, form='antilog'):\n",
    "        \"\"\"Method to calculate cumulative n-gram Maximum Likelihood Probability of a phrase or sentence.\"\"\"\n",
    "        words = self.tokenize(sentence)\n",
    "        P = 0\n",
    "        if n == 1:\n",
    "            for index, item in enumerate(words):\n",
    "                unigram = item\n",
    "\n",
    "                # laplace add-1\n",
    "                numerator = self.unigram[unigram] + 1\n",
    "                denominator = len(self.words) + len(self.unigram)\n",
    "\n",
    "                P += calc.log( numerator /denominator)\n",
    "\n",
    "        if n == 2:\n",
    "            for index, item in enumerate(words):\n",
    "                if index >= len(words) - 1:\n",
    "                    break\n",
    "                \n",
    "                unigram = item\n",
    "                bigram = item+' '+words[index+1]\n",
    "                \n",
    "                # laplace add-1\n",
    "                numerator = self.bigram[bigram] + 1\n",
    "                denominator = self.unigram[unigram] + len(self.unigram)\n",
    "\n",
    "                P += calc.log( numerator /denominator)\n",
    "\n",
    "        if n == 3:\n",
    "            for index, item in enumerate(words):\n",
    "                if index >= len(words) - 2:\n",
    "                    break\n",
    "             \n",
    "                bigram = item+' '+words[index+1]\n",
    "                trigram = item + ' ' + words[index+1] + ' ' + words[index+2]\n",
    "                \n",
    "                # laplace add-1\n",
    "                numerator = self.trigram[trigram] + 1\n",
    "                denominator = self.bigram[bigram] + len(self.unigram)\n",
    "\n",
    "                P += calc.log( numerator /denominator)\n",
    "\n",
    "\n",
    "        if n == 4:\n",
    "            for index, item in enumerate(words):\n",
    "                if index >= len(words) - 3:\n",
    "                    break\n",
    "\n",
    "                trigram = item + ' ' + words[index+1] + ' ' + words[index+2]\n",
    "                quadrigram = item + ' ' + words[index+1] + ' ' + words[index+2] + ' ' + words[index+3]\n",
    "\n",
    "                # laplace add-1\n",
    "                numerator = self.quadrigram[quadrigram] + 1\n",
    "                denominator = self.trigram[trigram] + len(self.unigram)\n",
    "\n",
    "                P += calc.log( numerator /denominator)\n",
    "\n",
    "        if n == 5:\n",
    "            for index, item in enumerate(words):\n",
    "                if index >= len(words) - 4:\n",
    "                    break\n",
    "\n",
    "                quadrigram = item + ' ' + words[index+1] + ' ' + words[index+2] + ' ' + words[index+3]\n",
    "                pentagram = item + ' ' + words[index+1] + ' ' + words[index+2] + ' ' + words[index+3] + ' ' + words[index+4]\n",
    "                \n",
    "                # laplace add-1\n",
    "                numerator = self.pentigram[pentagram] + 1\n",
    "                denominator = self.quadrigram[quadrigram] + len(self.unigram)\n",
    "\n",
    "                P += calc.log( numerator /denominator)\n",
    "\n",
    "        if form == 'log':\n",
    "            return P\n",
    "        elif form == 'antilog':\n",
    "            return calc.pow(calc.e, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Corpus from data file\n",
      "Processing Corpus\n",
      "Creating Unigram Model\n",
      "Calculating Count for Unigram Model\n",
      "Creating Bigram Model\n",
      "Calculating Count for Bigram Model\n",
      "Creating Trigram Model\n",
      "Calculating Count for Trigram Model\n",
      "Creating Quadrigram Model\n",
      "Calculating Count for Quadrigram Model\n",
      "Creating pentigram Model\n",
      "Calculating Count for pentigram Model\n",
      "Train                          Test                      Prob1Gram   Prob2Gram   Prob3Gram   Prob4Gram   Prob5Gram \n",
      "general_practice.csv           general_practice.csv           -63.043   -58.587   -55.387   -49.532   -42.928 \n",
      "general_practice.csv           medical_oncology.csv           -62.465   -55.010   -49.618   -43.148   -36.872 \n",
      "general_practice.csv           forensic_medicine.csv          -58.061   -52.331   -47.433   -40.774   -34.812 \n",
      "general_practice.csv           ophthalmology.csv              -65.275   -58.199   -52.981   -46.418   -39.924 \n",
      "general_practice.csv           surgery_plastic.csv            -76.232   -69.707   -64.893   -58.286   -51.762 \n",
      "general_practice.csv           infectious_disease_medicine.csv -74.384   -67.735   -62.779   -56.209   -49.652 \n",
      "general_practice.csv           neurology.csv                  -65.562   -58.227   -53.080   -46.517   -40.046 \n",
      "general_practice.csv           reproductive_medicine.csv      -79.330   -72.254   -67.077   -60.469   -53.942 \n",
      "general_practice.csv           psychiatry.csv                 -66.288   -61.272   -56.874   -50.400   -43.812 \n",
      "general_practice.csv           rheumatology.csv               -65.844   -59.048   -53.856   -47.339   -40.858 \n",
      "general_practice.csv           radiology.csv                  -65.535   -57.721   -52.544   -46.035   -39.679 \n",
      "general_practice.csv           immunochemistry.csv            -69.294   -62.126   -57.092   -50.601   -44.124 \n",
      "general_practice.csv           nephrology.csv                 -66.817   -58.951   -53.624   -47.090   -40.636 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0ad44371e4b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# pentigram probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mresult_pentigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mng_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_probability_mle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mlist_results_pentigam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_pentigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3d003d51784d>\u001b[0m in \u001b[0;36msentence_probability_mle\u001b[0;34m(self, sentence, n, form)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentence_probability_mle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'antilog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;34m\"\"\"Method to calculate cumulative n-gram Maximum Likelihood Probability of a phrase or sentence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3d003d51784d>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mi_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     return [\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     ]\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     return [\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     ]\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# We are not using CONTRACTIONS4 since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files_train = '/home/plubeda/git_repo/specialist-lexicon/03_treatment_text/dataframes/df_specialty_title_abstract/'\n",
    "files_test = '/home/plubeda/git_repo/specialist-lexicon/03_treatment_text/dataframes/df_specialty_title_abstract_case_report/'\n",
    "\n",
    "for file_train in os.listdir(files_train):\n",
    "\n",
    "    ng_train = nGramProbability(5, files_train + file_train)\n",
    "    \n",
    "    #print(\">> Training...\", file_train)\n",
    "    #with open(\"outputs/\" + file_train, \"w\") as fout:\n",
    "\n",
    "        # file header \n",
    "        #fout.write(\"Train Test Prob1gram Prob2gram Prob3gram Prob4gram Prob5gram\\n\")\n",
    "    print('{:30} {:25} {:6s}   {:6s}   {:6s}   {:6s}   {:6s} '.format(\"Train\", \"Test\", \"Prob1Gram\", \"Prob2Gram\", \"Prob3Gram\", \"Prob4Gram\", \"Prob5Gram\" ))\n",
    "    for file_test in os.listdir(files_test):\n",
    "\n",
    "        #print(\">> Testing...\", file_test)\n",
    "        \n",
    "        list_results_unigam = []\n",
    "        list_results_bigam = []\n",
    "        list_results_trigam = []\n",
    "        list_results_quadigam = []\n",
    "        list_results_pentigam = []\n",
    "\n",
    "        data_test = pd.read_csv(files_test + file_test)\n",
    "        list_titles = data_test.title.values\n",
    "       \n",
    "        for title in list_titles:\n",
    "\n",
    "            # pre-processing as in training (remove digit, remove punctuation, convert lowecase)\n",
    "            list_tokens = ng_train.tokenize(title)\n",
    "            list_tokens = [token for token in list_tokens if not token.isdigit()]\n",
    "            list_tokens = [token.lower() for token in list_tokens if not token in string.punctuation]\n",
    "            title = ' '.join(list_tokens)\n",
    "\n",
    "            # unigram probability\n",
    "            result_uigram = ng_train.sentence_probability_mle(sentence=title, n=1, form='log')\n",
    "            list_results_unigam.append(result_uigram)\n",
    "\n",
    "            # bigram probability\n",
    "            result_bigram = ng_train.sentence_probability_mle(sentence=title, n=2, form='log')\n",
    "            list_results_bigam.append(result_bigram)\n",
    "\n",
    "            # trigram probability\n",
    "            result_trigram = ng_train.sentence_probability_mle(sentence=title, n=3, form='log')\n",
    "            list_results_trigam.append(result_trigram)\n",
    "\n",
    "            # uadigram probability\n",
    "            result_quadigram = ng_train.sentence_probability_mle(sentence=title, n=4, form='log')\n",
    "            list_results_quadigam.append(result_quadigram)\n",
    "\n",
    "            # pentigram probability\n",
    "            result_pentigram = ng_train.sentence_probability_mle(sentence=title, n=5, form='log')\n",
    "            list_results_pentigam.append(result_pentigram)\n",
    "\n",
    "\n",
    "        unigram = sum(list_results_unigam)/len(list_results_unigam)\n",
    "        bigram = sum(list_results_bigam)/len(list_results_bigam)\n",
    "        trigram = sum(list_results_trigam)/len(list_results_trigam)\n",
    "        quadigram = sum(list_results_quadigam)/len(list_results_quadigam)\n",
    "        pentigram = sum(list_results_pentigam)/len(list_results_pentigam)\n",
    "\n",
    "        print('{:30} {:30} {:06.3f}   {:06.3f}   {:06.3f}   {:06.3f}   {:06.3f} '.format(file_train.split(\"_\", 1)[1], file_test.split(\"_\", 1)[1], unigram, bigram, trigram, quadigram, pentigram ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
